<div align="center">

  [![license](https://img.shields.io/github/license/new-soul-house/LLMPipeline.svg)](https://github.com/new-soul-house/LLMPipeline/tree/main/LICENSE)
  [![issue resolution](https://img.shields.io/github/issues-closed-raw/new-soul-house/LLMPipeline)](https://github.com/new-soul-house/LLMPipeline/issues)
  [![open issues](https://img.shields.io/github/issues-raw/new-soul-house/LLMPipeline)](https://github.com/new-soul-house/LLMPipeline/issues)

  <div>
    <a href="https://github.com/new-soul-house"><img src="https://img.shields.io/badge/NewSoul-æ–°å¿ƒéˆèˆ-blue" /></a>&emsp;
    <a href="https://linluhe.github.io/qrcode.html"><img src="https://img.shields.io/badge/WeChat-å¾®ä¿¡-07c160" /></a>&emsp;
    <a href="https://linluhe.github.io/group_qrcode.html"><img src="https://img.shields.io/badge/WeChat-å¾®ä¿¡ç¾¤-07c160" /></a>&emsp;
    <a href="https://space.bilibili.com/4557530/"><img src="https://img.shields.io/badge/Bilibili-Bç«™-ff69b4" /></a>&emsp;
    <!-- visitor statistics logo è®¿é—®é‡ç»Ÿè®¡å¾½æ ‡ -->
    <img src="https://komarev.com/ghpvc/?username=new-soul-house&label=Views&color=0e75b6&style=flat" alt="è®¿é—®é‡ç»Ÿè®¡" />
  </div>

  <p align="center">
    ğŸ‘‹ join us on <a href="https://linluhe.github.io/group_qrcode.html" target="_blank">WeChat</a>
  </p>
</div>

# LLMPipeline

## Introduction
LLMPipeline is a Python package designed to optimize the performance of tasks related to Large Language Models (LLMs). It ensures efficient parallel execution of tasks while maintaining dependency constraints, significantly enhancing the overall performance.

LLMPipeline æ˜¯ä¸€ä¸ª Python åŒ…ï¼Œæ—¨åœ¨ä¼˜åŒ–ä¸å¤§è¯­è¨€æ¨¡å‹ (LLM) ç›¸å…³ä»»åŠ¡çš„æ€§èƒ½ã€‚åœ¨æ»¡è¶³ä¾èµ–å…³ç³»çš„å‰æä¸‹ï¼Œç¡®ä¿ä»»åŠ¡çš„é«˜æ•ˆå¹¶è¡Œæ‰§è¡Œï¼Œä»è€Œæ˜¾è‘—æé«˜æ•´ä½“æ€§èƒ½ã€‚

## Features
- Dependency Management: Handles task dependencies efficiently, ensuring correct execution order.

  ä¾èµ–ç®¡ç†ï¼šé«˜æ•ˆå¤„ç†ä»»åŠ¡ä¾èµ–å…³ç³»ï¼Œç¡®ä¿æ­£ç¡®çš„æ‰§è¡Œé¡ºåºã€‚
- Parallel Execution: Maximizes parallelism to improve performance.

  å¹¶è¡Œæ‰§è¡Œï¼šæœ€å¤§åŒ–å¹¶è¡Œæ€§ä»¥æé«˜æ€§èƒ½ã€‚
- Loop Handling: Supports tasks with loop structures.

  å¾ªç¯å¤„ç†ï¼šæ”¯æŒå¸¦æœ‰å¾ªç¯ç»“æ„çš„ä»»åŠ¡ã€‚
- Easy Integration: Simple and intuitive API for easy integration with existing projects.

  æ˜“äºé›†æˆï¼šç®€å•ç›´è§‚çš„ APIï¼Œä¾¿äºä¸ç°æœ‰é¡¹ç›®é›†æˆã€‚

## Installation
You can install LLMPipeline via pip:

ä½ å¯ä»¥é€šè¿‡ pip å®‰è£… LLMPipelineï¼š
```bash
pip install llmpipeline
```

## Usage
Here is a basic example to get you started:

ä¸‹é¢æ˜¯ä¸€ä¸ªåŸºæœ¬ç¤ºä¾‹ï¼Œå¸®åŠ©ä½ å¿«é€Ÿå…¥é—¨ï¼š

```python
from llmpipeline import LLMPipeline, Prompt

# set custom prompt
example_prompt = Prompt("""
...
{inp1}
xxx
""", keys=['{inp1}'])

# set api
def llm_api(inp):
    ...
    return out

def rag_api(inp):
    ...
    return out

# set input data
data = {
    'inp': 'test input text ...',
}

# set pipeline
demo_pipe = {
    'process_input': {
        'mode': 'llm',
        'prompt': example_prompt,
        'format': {'out1': list, 'out2': str}, # check return json format
        'inp': ['inp'],
        'out': ['out1', 'out2'],
        'next': ['rag1', 'loop_A'], # specify the next pipeline
    },
    'rag1': {
        'mode': 'rag',
        'rag_backend': rag_api2, # specific api can be set for the current pipe via 'rag_backend' or 'llm_backend'.
        'inp': ['out2'],
        'out': 'out8',
    },
    'loop_A': { # here is iterating over a list 'out1'
        'mode': 'loop',
        'inp': 'out1',
        'pipe_in_loop': ['rag2', 'llm_process', 'rag3', 'rag4', 'llm_process2', 'llm_process3'],
        'next': ['exit'], # 'exit' is specific pipe mean to end
    },
    'rag2': {
        'mode': 'rag',
        'inp': ['out1'],
        'out': 'out3',
    },
    'llm_process2': {
        'mode': 'llm',
        'prompt': llm_process2_prompt,
        'format': {'xxx': str, "xxx": str},
        'inp': ['inp', 'out4', 'out8'],
        'out': 'final_out1',
    },
    ...
}

# running pipeline
pipeline = LLMPipeline(demo_pipe, data, llm_api, rag_api)
result, info = pipeline.run('process_input', core_num=4, save_pref=True)
```

Logs are stored in the `logs` folder. If `save_pref` is `true`, you can see the relevant performance report.

æ—¥å¿—å­˜å‚¨åœ¨logsæ–‡ä»¶å¤¹ä¸‹ï¼Œå¦‚æœsave_prefä¸ºtrueï¼Œä½ å¯ä»¥çœ‹åˆ°ç›¸å…³çš„æ€§èƒ½æŠ¥å‘Šã€‚

<div align="center">

  ![pipe](./assets/pipe.png)

  ![perf](./assets/perf.png)
</div>

## Documentation
For detailed documentation, please visit our official documentation page.

æœ‰å…³è¯¦ç»†æ–‡æ¡£ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„å®˜æ–¹æ–‡æ¡£é¡µé¢ã€‚

## Contributing
We welcome contributions from the community. Please read our contributing guide to get started.

æˆ‘ä»¬æ¬¢è¿æ¥è‡ªç¤¾åŒºçš„è´¡çŒ®ã€‚è¯·é˜…è¯»æˆ‘ä»¬çš„è´¡çŒ®æŒ‡å—å¼€å§‹ã€‚

## License
LLMPipeline is licensed under the Apache License Version 2.0. See the [LICENSE](./LICENSE) file for more details.

LLMPipeline é‡‡ç”¨ Apache License Version 2.0 è®¸å¯è¯ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[è®¸å¯è¯](./LICENSE)æ–‡ä»¶ã€‚

## Acknowledgements
Special thanks to all contributors and the open-source community for their support.

ç‰¹åˆ«æ„Ÿè°¢æ‰€æœ‰è´¡çŒ®è€…å’Œå¼€æºç¤¾åŒºçš„æ”¯æŒã€‚

## Contact
For any questions or issues, please open an issue on our [GitHub repository](https://github.com/new-soul-house/LLMPipeline).

å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–æ„è§ï¼Œè¯·åœ¨æˆ‘ä»¬çš„[GitHub ä»“åº“](https://github.com/new-soul-house/LLMPipeline)æäº¤ issueã€‚

<div align="center">
  
[![Star History Chart](https://api.star-history.com/svg?repos=new-soul-house/LLMPipeline&type=Date)](https://star-history.com/#new-soul-house/LLMPipeline&Date)

</div>
